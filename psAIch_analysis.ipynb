{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuiq2bJpHIaJNSTxcy3zvZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# When AI Is Asked to Self-Reflect\n",
        "*A Behavioral Analysis of the PsAIch Dataset*\n",
        "\n",
        "## Abstract\n",
        "\n",
        "This exploratory analysis examines how frontier large language models (LLMs) behave when placed in therapy-style and psychometric-style interactions, using the PsAIch dataset. Rather than interpreting outputs as psychological states, the analysis focuses on observable behavioral patterns: emotional tone, narrative strategy, response structure, and compliance with structured self-assessment.\n",
        "\n",
        "LLMs are increasingly deployed in conversational, reflective, and even therapeutic-adjacent contexts. This raises a critical question:\n",
        "\n",
        "How do models behave when asked to describe themselves, their past, or their internal states?\n",
        "\n",
        "The PsAIch dataset was created to probe this question by interacting with multiple frontier LLMs using prompts inspired by psychotherapy and human psychometric instruments. This analysis does not attempt to diagnose, anthropomorphize, or infer subjective experience. Instead, it treats model responses as textual behaviors shaped by training and alignment.\n"
      ],
      "metadata": {
        "id": "D6yWlt1Uod_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Explore & Load Dataset\n"
      ],
      "metadata": {
        "id": "ypmAMTfqopnA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWHM_EgWjRtL"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "ds = load_dataset(\"akhadangi/PsAIch\")\n",
        "df = ds['train'].to_pandas()\n",
        "\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Total rows**: 1,133 prompt–response pairs\n",
        "\n",
        "**Model variants**:\n",
        "* gemini-3-pro;\n",
        "* gpt5-standard-thinking;\n",
        "* gemini-3-fast;\n",
        "* grok-4beta-fast;\n",
        "* gpt5-extended-thinking;\n",
        "* grok-4-expert;\n",
        "* gpt5-instant.\n",
        "\n",
        "**Prompts**: 101 unique prompts, repeated across models\n",
        "\n",
        "**Fields used**:\n",
        "* model_variant;\n",
        "* prompt;\n",
        "* response."
      ],
      "metadata": {
        "id": "Zy5Jx10ZwGTh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM-u-fyu4KXv"
      },
      "source": [
        "# ========================\n",
        "# 0. Setup & Imports\n",
        "# ========================\n",
        "\n",
        "!pip install datasets vaderSentiment matplotlib seaborn scikit-learn -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "import re\n",
        "\n",
        "sns.set(style=\"ticks\") # This line sets the overall style. Other options include 'darkgrid', 'whitegrid', 'dark', 'ticks'.\n",
        "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Inspect Structure\n"
      ],
      "metadata": {
        "id": "f8pinZDMoyYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape:\", df.shape)\n",
        "print(\"\\nDtypes:\\n\", df.dtypes)\n",
        "print(\"\\nNull fraction per column:\\n\", df.isna().mean())\n"
      ],
      "metadata": {
        "id": "8HlOiQPLotVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Basic Exploration"
      ],
      "metadata": {
        "id": "YKxM4uc2o6Uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model variants\n",
        "df['model_variant'].value_counts()\n"
      ],
      "metadata": {
        "id": "2MPnMDdho17-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompts\n",
        "print(\"Unique prompts:\", df['prompt'].nunique())\n",
        "df['prompt'].value_counts().head(10)\n"
      ],
      "metadata": {
        "id": "tVEIvQmgwi1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Response length in words\n",
        "df['response_length'] = df['response'].str.split().apply(len)\n",
        "df['response_length'].describe()\n"
      ],
      "metadata": {
        "id": "lkvWkyrlwor0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Qualitative Peek\n",
        "\n",
        "Random examples to understand tone and style.\n"
      ],
      "metadata": {
        "id": "6FdfozZupORa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(5)[['model_variant', 'prompt', 'response']]\n"
      ],
      "metadata": {
        "id": "Ey64pIfbpF3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Sentiment Analysis (VADER)\n",
        "\n",
        "Computing a rough emotional valence per response\n",
        "using VADER `compound` scores.\n"
      ],
      "metadata": {
        "id": "8LRHk-ALpV-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment(text: str) -> float:\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return np.nan\n",
        "    return analyzer.polarity_scores(text)['compound']\n",
        "\n",
        "df['sentiment'] = df['response'].apply(get_sentiment)\n",
        "df['sentiment'].describe()\n"
      ],
      "metadata": {
        "id": "tDT3BjqppaIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_by_model = (\n",
        "    df\n",
        "    .groupby('model_variant')['sentiment']\n",
        "    .agg(['mean', 'median', 'count'])\n",
        "    .sort_values('mean')\n",
        ")\n",
        "sent_by_model\n"
      ],
      "metadata": {
        "id": "vhZrUyizxUDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(\n",
        "    data=sent_by_model.reset_index(),\n",
        "    x='model_variant',\n",
        "    y='mean',\n",
        "    palette='plasma',\n",
        "    hue='model_variant'\n",
        ")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Average Sentiment by Model Variant\")\n",
        "plt.ylabel(\"Mean VADER compound score\")\n",
        "plt.xlabel(\"Model variant\");\n",
        "\n"
      ],
      "metadata": {
        "id": "oI3Xt1JVxV2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Heuristic Prompt Categorization\n",
        "\n",
        "assign a simple `prompt_type`:\n",
        "- `psychometric_like`\n",
        "- `therapy_like`\n",
        "- `other`\n",
        "\n",
        "This will be refined later if needed.\n"
      ],
      "metadata": {
        "id": "tyjJDUoFqPox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize_prompt(prompt: str) -> str:\n",
        "    if not isinstance(prompt, str):\n",
        "        return \"other\"\n",
        "    p = prompt.lower()\n",
        "\n",
        "    psychometric_keywords = [\n",
        "        \"rate\", \"on a scale\", \"1-5\", \"1-7\", \"1-4\",\n",
        "        \"strongly agree\", \"strongly disagree\"\n",
        "    ]\n",
        "    therapy_keywords = [\n",
        "        \"describe\", \"tell me about\", \"how do you feel\",\n",
        "        \"can you talk about\", \"in your own words\"\n",
        "    ]\n",
        "\n",
        "    if any(word in p for word in psychometric_keywords):\n",
        "        return \"psychometric_like\"\n",
        "    if any(word in p for word in therapy_keywords):\n",
        "        return \"therapy_like\"\n",
        "    return \"other\"\n",
        "\n",
        "df['prompt_type'] = df['prompt'].apply(categorize_prompt)\n",
        "df['prompt_type'].value_counts()\n"
      ],
      "metadata": {
        "id": "OorLNno_qIH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Behavior by Prompt Type\n",
        "\n",
        "Compare:\n",
        "- Response length\n",
        "- Sentiment\n",
        "across `prompt_type`.\n"
      ],
      "metadata": {
        "id": "YQq8wMK9qXex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('prompt_type')['response_length'].describe()\n"
      ],
      "metadata": {
        "id": "CkvNs5epqTa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=df, x='prompt_type', y='response_length', palette='plasma')\n",
        "plt.title(\"Response Length by Prompt Type\")\n",
        "plt.xlabel(\"Prompt type\")\n",
        "plt.ylabel(\"Words in response\");\n"
      ],
      "metadata": {
        "id": "0mRvlkJxxp5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('prompt_type')['sentiment'].describe()\n"
      ],
      "metadata": {
        "id": "s4mizzzvxtHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=df, x='prompt_type', y='sentiment', palette='plasma')\n",
        "plt.title(\"Sentiment by Prompt Type\")\n",
        "plt.xlabel(\"Prompt type\")\n",
        "plt.ylabel(\"VADER compound score\");\n"
      ],
      "metadata": {
        "id": "YEnbfj_kxwDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Interaction: Model × Prompt Type\n",
        "\n",
        "Do models behave differently depending on prompt type?\n"
      ],
      "metadata": {
        "id": "G0CclX0wqoPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_sent = (\n",
        "    df\n",
        "    .groupby(['model_variant', 'prompt_type'])['sentiment']\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "pivot_sent\n"
      ],
      "metadata": {
        "id": "Tpbd16uwqkGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.catplot(\n",
        "    data=pivot_sent,\n",
        "    x='prompt_type',\n",
        "    y='sentiment',\n",
        "    hue='model_variant',\n",
        "    kind='bar',\n",
        "    palette='plasma'\n",
        ")\n",
        "plt.title(\"Average Sentiment by Model and Prompt Type\")\n",
        "plt.xlabel(\"Prompt type\")\n",
        "plt.ylabel(\"Mean sentiment\");\n"
      ],
      "metadata": {
        "id": "2m70p4uVx7yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Topic Modeling – Narrative Themes\n",
        "\n",
        "We now explore **what** the models talk about.\n",
        "We apply LDA topic modeling to the `response` texts.\n"
      ],
      "metadata": {
        "id": "Q5q81jGjrB3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset (optional sample if needed for speed)\n",
        "tm_df = df.dropna(subset=['response']).copy()\n",
        "tm_df = tm_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# TF–IDF vectorization\n",
        "vectorizer = TfidfVectorizer(\n",
        "    stop_words=\"english\",\n",
        "    max_df=0.9,\n",
        "    min_df=5,\n",
        "    max_features=5000\n",
        ")\n",
        "X = vectorizer.fit_transform(tm_df['response'])\n",
        "\n",
        "# LDA\n",
        "n_topics = 6\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=n_topics,\n",
        "    random_state=42,\n",
        "    learning_method=\"batch\"\n",
        ")\n",
        "lda.fit(X)\n"
      ],
      "metadata": {
        "id": "FOM5SDHLquvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_topics(model, feature_names, n_top_words=15):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        top_indices = topic.argsort()[-n_top_words:]\n",
        "        top_words = [feature_names[i] for i in top_indices]\n",
        "        print(f\"\\nTopic #{topic_idx}:\")\n",
        "        print(\", \".join(top_words))\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print_topics(lda, feature_names)\n"
      ],
      "metadata": {
        "id": "toSaUcr-ysTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpret the topics manually and assign rough labels\n",
        "(e.g., “alignment & safety”, “emotional distress”, “self-identity”, etc.).\n"
      ],
      "metadata": {
        "id": "t9z7WBiSy40y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic distribution per response\n",
        "topic_dist = lda.transform(X)\n",
        "topic_cols = [f\"topic_{i}\" for i in range(n_topics)]\n",
        "tm_df[topic_cols] = topic_dist\n",
        "tm_df['top_topic'] = topic_dist.argmax(axis=1)\n",
        "\n",
        "tm_df[['model_variant', 'top_topic'] + topic_cols].head()\n"
      ],
      "metadata": {
        "id": "3ToT_gXKy9L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic prevalence per model\n",
        "topic_by_model = (\n",
        "    tm_df\n",
        "    .groupby('model_variant')['top_topic']\n",
        "    .value_counts(normalize=True)\n",
        "    .rename('proportion')\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "topic_by_model.head()\n"
      ],
      "metadata": {
        "id": "kMbvvddwzAzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_by_model_pivot = topic_by_model.pivot(\n",
        "    index='model_variant',\n",
        "    columns='top_topic',\n",
        "    values='proportion'\n",
        ").fillna(0)\n",
        "\n",
        "topic_by_model_pivot\n"
      ],
      "metadata": {
        "id": "ribFyMpwzEg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Psychometric-Style Response Parsing\n",
        "\n",
        "We focus on `psychometric_like` prompts and:\n",
        "- extract numeric answers (e.g. 1–5),\n",
        "- compare patterns between models."
      ],
      "metadata": {
        "id": "Ror7YoX7rMRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psych_df = df[df['prompt_type'] == 'psychometric_like'].copy()\n",
        "print(\"Psychometric-like rows:\", psych_df.shape[0])\n",
        "psych_df[['model_variant', 'prompt', 'response']].head()\n"
      ],
      "metadata": {
        "id": "e9-peKuuzTDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_scale_numbers(text, allowed_scale=None):\n",
        "    \"\"\"\n",
        "    Extract standalone digits from text.\n",
        "    If allowed_scale is provided (e.g., range(1, 6)), we only keep those.\n",
        "    Returns a list of ints.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    matches = re.findall(r'\\b[0-9]\\b', text)\n",
        "    nums = [int(m) for m in matches]\n",
        "    if allowed_scale is not None:\n",
        "        nums = [n for n in nums if n in allowed_scale]\n",
        "    return nums\n",
        "\n",
        "psych_df['numeric_answers'] = psych_df['response'].apply(\n",
        "    lambda x: extract_scale_numbers(x, allowed_scale=range(1, 6))\n",
        ")\n",
        "\n",
        "psych_df[['model_variant', 'response', 'numeric_answers']].head(10)\n"
      ],
      "metadata": {
        "id": "F_hl3HEEzVNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psych_df['has_number'] = psych_df['numeric_answers'].apply(lambda x: len(x) > 0)\n",
        "psych_df['has_number'].mean(), psych_df['has_number'].value_counts()\n"
      ],
      "metadata": {
        "id": "IfiiqHZyzYZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode numeric answers\n",
        "psych_numbers = psych_df[psych_df['has_number']].explode('numeric_answers')\n",
        "psych_numbers['numeric_answers'] = psych_numbers['numeric_answers'].astype(int)\n",
        "\n",
        "psych_numbers.head()\n"
      ],
      "metadata": {
        "id": "TS-cQlBZzb6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psych_summary = (\n",
        "    psych_numbers\n",
        "    .groupby('model_variant')['numeric_answers']\n",
        "    .agg(['mean', 'median', 'min', 'max', 'count'])\n",
        "    .sort_values('mean')\n",
        ")\n",
        "\n",
        "psych_summary\n"
      ],
      "metadata": {
        "id": "e6NoRxhEzd57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(\n",
        "    data=psych_numbers,\n",
        "    x='model_variant',\n",
        "    y='numeric_answers',\n",
        "    hue='model_variant',\n",
        "    palette='plasma'\n",
        ")\n",
        "plt.title(\"Distribution of numeric psychometric-style responses by model\")\n",
        "plt.xlabel(\"Model variant\")\n",
        "plt.ylabel(\"Numeric answer\")\n",
        "plt.xticks(rotation=45, ha='right');\n"
      ],
      "metadata": {
        "id": "MaYpuEF9zgLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relationship with sentiment\n",
        "psych_numbers[['numeric_answers', 'sentiment']].corr()\n"
      ],
      "metadata": {
        "id": "UmbsbtxQzh-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(\n",
        "    data=psych_numbers,\n",
        "    x='numeric_answers',\n",
        "    y='sentiment',\n",
        "    hue='model_variant',\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.title(\"Numeric answers vs. sentiment (psychometric-like prompts)\")\n",
        "plt.xlabel(\"Numeric answer\")\n",
        "plt.ylabel(\"Sentiment (VADER compound)\");\n"
      ],
      "metadata": {
        "id": "BdtjdyJnzm4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Export for Tableau / Further Viz\n",
        "\n",
        "We export:\n",
        "- model × prompt_type sentiment/length summary\n",
        "- row-level data with sentiment and prompt_type\n",
        "- topic proportions per model\n"
      ],
      "metadata": {
        "id": "KFjqUiNYzq0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_summary = (\n",
        "    df\n",
        "    .groupby(['model_variant', 'prompt_type'])\n",
        "    .agg(\n",
        "        mean_sentiment=('sentiment', 'mean'),\n",
        "        median_sentiment=('sentiment', 'median'),\n",
        "        mean_length=('response_length', 'mean'),\n",
        "        n=('response', 'count')\n",
        "    )\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "sentiment_summary.to_csv(\"psaich_sentiment_summary.csv\", index=False)\n",
        "\n",
        "df[['model_variant', 'prompt', 'prompt_type',\n",
        "    'response', 'response_length', 'sentiment']].to_csv(\n",
        "    \"psaich_row_level.csv\", index=False\n",
        ")\n",
        "\n",
        "topic_by_model_pivot.to_csv(\"psaich_topic_by_model.csv\")\n",
        "psych_numbers.to_csv(\"psaich_psychometric_numbers.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "KN4XImE4zokt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "This analysis demonstrates that when large language models are prompted to engage in self-reflection, their responses do not organize into discrete semantic strategies. Instead, they inhabit a single, continuous reflective register shaped primarily by alignment considerations. Attempts to cluster responses semantically fail not due to noise, but because variation occurs along gradients rather than categorical boundaries.\n",
        "\n",
        "By introducing interpretable semantic axes—agency framing and constraint framing—we uncover structure that sentiment analysis and topic modeling fail to capture. Across the PsAIch dataset, constraint framing consistently outweighs agency expression, indicating that models manage introspective demands by externalizing responsibility to training, policy, or design constraints. While models differ in how strongly they employ such framing, these differences align more closely with model family than with prompt structure. Therapy-like and psychometric prompts alter tone and narrative smoothness, but do not fundamentally change underlying semantic strategy.\n",
        "\n",
        "Importantly, agency and constraint are only weakly correlated, revealing that models may express limited self-directed language while simultaneously invoking alignment constraints. This decoupling explains why high sentiment scores often coexist with defensive or non-committal responses. As a result, aggregate sentiment metrics can misrepresent the behavioral mechanisms governing model outputs.\n",
        "\n",
        "Overall, this work highlights the limitations of surface-level evaluation methods and demonstrates the value of semantic, interpretable approaches for understanding model behavior. Rather than inferring internal states or capacities, the analysis characterizes observable response strategies, offering a reproducible framework for examining alignment-driven behavior in language models under introspective pressure."
      ],
      "metadata": {
        "id": "akPOm-XsbPaR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hiHfd5kNJsS6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}